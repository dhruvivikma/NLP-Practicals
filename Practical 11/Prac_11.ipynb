{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prac_11.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vD9gd5xEqq4g"
      },
      "source": [
        "PRACTICAL 11"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1l3n_65qsQA",
        "outputId": "fc6df342-41ba-4aaa-bdd5-677e55e7cab8"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "nltk.download('brown')\n",
        "brown.tagged_sents()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')], [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')], ...]"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HC8zzYvirLlV",
        "outputId": "5545ec94-61d0-4103-c3c7-8adc4d4256dc"
      },
      "source": [
        "# list of all the unique tags from the corpus\n",
        "\n",
        "brown_word_tags=[]\n",
        "\n",
        "#Manually adding the start and the end tag\n",
        "for brown_sent in brown.tagged_sents():\n",
        "    brown_word_tags.append(('START','START'))\n",
        "    \n",
        "    for words,tag in brown_sent:\n",
        "        brown_word_tags.extend([(tag[:2],words)])\n",
        "        \n",
        "    brown_word_tags.append(('END','END'))\n",
        "#Getting the continuous frequency distribution for the words which are tagged\n",
        "cfd_tag_words=nltk.ConditionalFreqDist(brown_word_tags)\n",
        "#Getting the conditional probability distribution\n",
        "cpd_tag_words=nltk.ConditionalProbDist(cfd_tag_words,nltk.MLEProbDist)\n",
        "print(\"The probability of an adjective (JJ) being 'smart' is\", cpd_tag_words[\"JJ\"].prob(\"smart\"))\n",
        "print(\"The probability of a verb (VB) being 'try' is\", cpd_tag_words[\"VB\"].prob(\"try\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The probability of an adjective (JJ) being 'smart' is 0.00027780092785509904\n",
            "The probability of a verb (VB) being 'try' is 0.0010790559555256297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kZbXMvUrMzH",
        "outputId": "86d735a8-74d6-4424-d880-0342fa10df26"
      },
      "source": [
        "brown_tags=[]\n",
        "for tag, words in brown_word_tags:\n",
        "    brown_tags.append(tag)\n",
        "#make conditional frequency distribution: count(t{i-1} ti)\n",
        "cfd_tags=nltk.ConditionalFreqDist(nltk.bigrams(brown_tags))\n",
        "# make conditional probability distribution, using maximum likelihood estimate: P(ti | t{i-1})\n",
        "cpd_tags=nltk.ConditionalProbDist(cfd_tags,nltk.MLEProbDist)\n",
        "print('The probability of DT occuring after NN is : ', cpd_tags[\"NN\"].prob(\"DT\"))\n",
        "print('The probability of VB occuring after NN is : ', cpd_tags[\"NN\"].prob(\"VB\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The probability of DT occuring after NN is :  0.0018349509874933604\n",
            "The probability of VB occuring after NN is :  0.0646359290427087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DM5RsFPrP2y",
        "outputId": "c3f8776e-cc3d-4bbd-a9a0-0ddd39dcb69d"
      },
      "source": [
        "# Implementation of Viterbi algorithm\n",
        "distinct_brown_tags=set(brown_tags)\n",
        "sample_sentences=[\"I\",\"love\",\"watching\",\"sunsets\"]\n",
        "len_sample_sentence=len(sample_sentences)\n",
        "viterbi_tags={}\n",
        "viterbi_backpointer={}\n",
        "\n",
        "for tag in distinct_brown_tags:\n",
        "    if tag==\"START\":\n",
        "        continue\n",
        "    viterbi_tags[tag]=cpd_tags[\"START\"].prob(tag)*cpd_tag_words[tag].prob(sample_sentences[0])\n",
        "    viterbi_backpointer[tag]=\"START\"\n",
        "# for each step i in 1 .. sentlen,\n",
        "# store a dictionary\n",
        "# that maps each tag X\n",
        "# to the probability of the best tag sequence of length i that ends in X\n",
        "\n",
        "\n",
        "\n",
        "viterbi_main=[]\n",
        "backpointer_main=[]\n",
        "\n",
        "viterbi_main.append(viterbi_tags)\n",
        "backpointer_main.append(viterbi_backpointer)\n",
        "\n",
        "current_best=max(viterbi_tags.keys(),key=lambda tag: viterbi_tags[tag])\n",
        "print(\"Word\", \"'\" + sample_sentences[0] + \"'\", \"current best two-tag sequence:\", viterbi_backpointer[current_best], current_best)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word 'I' current best two-tag sequence: START PP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TL-fKPtNrXsV",
        "outputId": "74539cc0-cdfd-40c4-f3a9-b7f3dfc2f046"
      },
      "source": [
        "for index in range(1,len_sample_sentence):\n",
        "    curr_viterbi={}\n",
        "    curr_backpointer={}\n",
        "    prev_viterbi=viterbi_main[-1]\n",
        "    \n",
        "    for brown_tag in distinct_brown_tags:\n",
        "        \n",
        "        if brown_tag != \"START\":\n",
        "            # if this tag is X and the current word is w, then\n",
        "            # find the previous tag Y such that\n",
        "            # the best tag sequence that ends in X\n",
        "            # actually ends in Y X\n",
        "            # that is, the Y that maximizes\n",
        "            # prev_viterbi[ Y ] * P(X | Y) * P( w | X)\n",
        "            # The following command has the same notation\n",
        "            # that you saw in the sorted() command.\n",
        "            prev_best = max(prev_viterbi.keys(),\n",
        "                                key=lambda prevtag: \\\n",
        "                                    prev_viterbi[prevtag] * cpd_tags[prevtag].prob(brown_tag) * cpd_tag_words[brown_tag].prob(\n",
        "                                        sample_sentences[index]))\n",
        "\n",
        "            curr_viterbi[brown_tag] = prev_viterbi[prev_best] * \\\n",
        "                                cpd_tags[prev_best].prob(brown_tag) * cpd_tag_words[brown_tag].prob(sample_sentences[index])\n",
        "            curr_backpointer[brown_tag] = prev_best\n",
        "\n",
        "    current_best = max(curr_viterbi.keys(), key=lambda tag: curr_viterbi[tag])\n",
        "    print(\"Word\", \"'\" + sample_sentences[index] + \"'\", \"current best two-tag sequence:\", curr_backpointer[current_best], current_best)\n",
        "\n",
        "\n",
        "    viterbi_main.append(curr_viterbi)\n",
        "    backpointer_main.append(curr_backpointer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word 'love' current best two-tag sequence: PP NN\n",
            "Word 'watching' current best two-tag sequence: NN VB\n",
            "Word 'sunsets' current best two-tag sequence: END END\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0e-HTqfrlF6",
        "outputId": "1467e152-b417-4caf-ebfb-11b57a4e0080"
      },
      "source": [
        "prev_viterbi = viterbi_main[-1]\n",
        "prev_best = max(prev_viterbi.keys(),\n",
        "                    key=lambda prev_tag: prev_viterbi[prev_tag] * cpd_tags[prev_tag].prob(\"END\"))\n",
        "\n",
        "prob_tag_sequence = prev_viterbi[prev_best] * cpd_tags[prev_best].prob(\"END\")\n",
        "\n",
        "\n",
        "best_tag_sequence = [\"END\", prev_best]\n",
        "# invert the list of backpointers\n",
        "backpointer_main.reverse()\n",
        "\n",
        "# go backwards through the list of backpointers\n",
        "# (or in this case forward, because we have inverter the backpointer list)\n",
        "# in each case:\n",
        "# the following best tag is the one listed under\n",
        "# the backpointer for the current best tag\n",
        "current_best_tag = prev_best\n",
        "for backpointer in backpointer_main:\n",
        "    best_tag_sequence.append(backpointer[current_best_tag])\n",
        "    current_best_tag = backpointer[current_best_tag]\n",
        "best_tag_sequence.reverse()\n",
        "print(\"The sentence given is :\")\n",
        "for word in sample_sentences:\n",
        "    print (word,\"\",)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentence given is :\n",
            "I \n",
            "love \n",
            "watching \n",
            "sunsets \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FeW6gZjrmLh",
        "outputId": "a01eb41c-d992-4547-884d-cf2288435ab1"
      },
      "source": [
        "print(\"The best tag sequence using HMM for the given sentence is : \")\n",
        "\n",
        "\n",
        "for best_tag in best_tag_sequence:\n",
        "    print (best_tag, \"\",)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best tag sequence using HMM for the given sentence is : \n",
            "START \n",
            "END \n",
            "END \n",
            "END \n",
            "END \n",
            "END \n"
          ]
        }
      ]
    }
  ]
}